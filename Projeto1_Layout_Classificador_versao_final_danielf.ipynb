{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Daniel Vergamini Luna  Frussa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atenção: Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re \n",
    "import emoji\n",
    "import functools\n",
    "import operator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "C:\\Users\\Ana\\Documents\\DANIEL\\Insper\\SEGUNDO SEMESTRE\\Ciencia de Dados\\projetos\\Projeto1\\Entrega Final\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diretório')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e não relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrei o arquivo roomba.xlsx, tudo certo\n"
     ]
    }
   ],
   "source": [
    "filename = 'roomba.xlsx'\n",
    "\n",
    "if filename in os.listdir():\n",
    "    print(f'Encontrei o arquivo {filename}, tudo certo')\n",
    "else:\n",
    "    print(f'Não encontrei o arquivo {filename} aqui no diretório {os.getcwd()}, será que você não baixou o arquivo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino = pd.read_excel(filename, sheet_name = 'Treinamento')\n",
    "teste = pd.read_excel(filename, sheet_name = 'Teste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'about', 'according', 'actually', 'after', 'all', 'also', 'always', 'am', 'an', 'and', 'any', 'anyone', 'are', 'around', 'at', 'away', 'bc', 'be', 'because', 'been', 'being', 'but', 'by', 'can', 'cannot', 'cant', 'could', 'did', 'do', 'does', 'doing', 'even', 'ever', 'every', 'everywhere', 'find', 'first', 'for', 'found', 'from', 'get', 'getting', 'going', 'got', 'had', 'has', 'have', 'having', 'he', 'her', 'hes', 'him', 'his', 'how', 'i3', 'if', 'im', 'in', 'into', 'is', 'it', 'its', 'just', 'keeps', 'know', 'later', 'less', 'like', 'little', 'look', 'made', 'make', 'makes', 'many', 'maybe', 'me', 'mine', 'more', 'my', 'name', 'need', 'needs', 'never', 'new', 'next', 'no', 'not', 'now', 'of', 'off', 'oh', 'on', 'once', 'one', 'only', 'or', 'otherwise', 'our', 'out', 'over', 'own', 'put', 'ran', 'said', 'saw', 'saying', 'says', 'see', 'seen', 'she', 'should', 'so', 'some', 'someone', 'something', 'sometimes', 'soon', 'still', 'take', 'than', 'that', 'thats', 'the', 'their', 'them', 'then', 'there', 'theres', 'they', 'theyre', 'think', 'this', 'those', 'though', 'through', 'to', 'too', 'trying', 'two', 'up', 'us', 'very', 'via', 'want', 'was', 'way', 'we', 'well', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'why', 'will', 'with', 'wonder', 'would', 'yes', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "#Função de limpeza que troca alguns sinais básicos por espaços \n",
    "def cleanup(text):\n",
    "    #Retira as sequências de controle essa linha de código é fundamental para construção das bases de dados já que\n",
    "    #se essas sequencias de controle não forem reiradas, o compilador pode acabar embaralhando os \"pd.series\" a partir\n",
    "    #desses comandos\n",
    "    text = text.replace(\"\\\\n\", '').replace(\"\\n\\n\", '').replace(\"\\n\", '').replace(\"[\", '').replace(\"]\", '')\n",
    "    \n",
    "    \n",
    "    #Função de limpeza muito simples que troca alguns sinais básicos por espaços\n",
    "    punctuation = '[!-..:?;_—•\\/|‘’“”%#&༉@‧₱₊˚~$]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', text)\n",
    "    \n",
    "    text_subbed = text_subbed.replace(\"rt\", '').replace(\"https\", '')\n",
    "    return text_subbed\n",
    "    \n",
    "    return text_subbed\n",
    "\n",
    "#Função que auxilia na limpesa do material analizado, separando emojis de outros carácteres.        \n",
    "def separa_emoji(lista):\n",
    "    lista_split_emoji = emoji.get_emoji_regexp().split(lista)\n",
    "    lista_split_whitespace = [substr.split() for substr in lista_split_emoji]\n",
    "    lista_split = functools.reduce(operator.concat, lista_split_whitespace)\n",
    "    return lista_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#lista de 'stopwords_auxiliar\" é uma lista de stopword muito completa que contém palavras que não influenciam a relevância\n",
    "#dos tweets\n",
    "stopwords_aux = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\",\n",
    "             \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\",\n",
    "             \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\",\n",
    "             \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\",\n",
    "             \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\",\n",
    "             \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\",\n",
    "             \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\",\n",
    "             \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\",\n",
    "             \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\",\n",
    "             \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\",\n",
    "             \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\",\n",
    "             \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\",\n",
    "             \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\",\n",
    "             \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\",\n",
    "             \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\",\n",
    "             \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\",\n",
    "             \"cr\", \"cry\", \"cs\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\",\n",
    "             \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\",\n",
    "             \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\",\n",
    "             \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\",\n",
    "             \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \n",
    "             \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\",\n",
    "             \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\",\n",
    "             \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \n",
    "             \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\",\n",
    "             \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\",\n",
    "             \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\",\n",
    "             \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\",\n",
    "             \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\",\n",
    "             \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\",\n",
    "             \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\",\n",
    "             \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\",\n",
    "             \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\",\n",
    "             \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\",\n",
    "             \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\",\n",
    "             \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\",\n",
    "             \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\",\n",
    "             \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\",\n",
    "             \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\",\n",
    "             \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\",\n",
    "             \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\",\n",
    "             \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\",\n",
    "             \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\",\n",
    "             \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\",\n",
    "             \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\",\n",
    "             \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\",\n",
    "             \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\",\n",
    "             \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\",\n",
    "             \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\",\n",
    "             \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\",\n",
    "             \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\",\n",
    "             \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\",\n",
    "             \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\",\n",
    "             \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\",\n",
    "             \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\",\n",
    "             \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\",\n",
    "             \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\",\n",
    "             \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\",\n",
    "             \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \n",
    "             \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\",\n",
    "             \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\",\n",
    "             \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\",\n",
    "             \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\",\n",
    "             \"she's\", \"should\", \"shouldnt\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\",\n",
    "             \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\",\n",
    "             \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\",\n",
    "             \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\",\n",
    "             \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\",\n",
    "             \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\",\n",
    "             \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\",\n",
    "             \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\",\n",
    "             \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\",\n",
    "             \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\",\n",
    "             \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\",\n",
    "             \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\",\n",
    "             \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\",\n",
    "             \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\",\n",
    "             \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\",\n",
    "             \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\",\n",
    "             \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\",\n",
    "             \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\",\n",
    "             \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"went\",\n",
    "             \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\",\n",
    "             \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\",\n",
    "             \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\",\n",
    "             \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"wi\", \"widely\", \"will\",\n",
    "             \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\",\n",
    "             \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\",\n",
    "             \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\",\n",
    "             \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",]\n",
    "\n",
    "\n",
    "#Como o compilador dá \"key error\" caso uma palavra na lista stopwords não esteja presente nas bases de dados do trabalho\n",
    "#usei essa lista abaixo para excluir as palavras não presentes na base de dados da lista das stopwords\n",
    "\n",
    "\n",
    "stopwords_lixo = [\"a\",\"0o\",\"0s\",\"3a\",\"3b\",\"3d\",\"6b\",\"6o\",\"a1\",\"a2\",\"a3\",\"a4\",\"ab\",\"above\",\"abst\",\"ac\",\"accordance\",\"accordingly\",\n",
    "                \"across\",\"act\",\"added\",\"adj\",\"ae\",\"af\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ag\",\"against\",\"ah\",\n",
    "                \"ain\",\"ain't\",\"aj\",\"al\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"although\",\"among\",\"amongst\",\n",
    "                \"amoungst\",\"amount\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anything\",\"anyway\",\"anyways\",\n",
    "                \"anywhere\",\"ao\",\"ap\",\"apart\",\"apparently\",\"appear\",\"appreciate\",\"appropriate\",\"approximately\",\"ar\",\"aren\",\n",
    "                \"aren't\",\"arise\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"au\",\"auth\",\"av\",\"available\",\"aw\",\"awfully\",\"ax\",\n",
    "                \"ay\",\"az\",\"b\",\"b1\",\"b2\",\"b3\",\"ba\",\"back\",\"bd\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\n",
    "                \"beginning\",\"beginnings\",\"behind\",\"believe\",\"below\",\"beside\",\"besides\",\"between\",\"beyond\",\"bi\",\"bill\",\n",
    "                \"biol\",\"bj\",\"bk\",\"bl\",\"bn\",\"both\",\"bottom\",\"bp\",\"br\",\"brief\",\"briefly\",\"bs\",\"bt\",\"bu\",\"bx\",\"c\",\"c1\",\"c2\",\n",
    "                \"c3\",\"ca\",\"can't\",\"cause\",\"causes\",\"cc\",\"cd\",\"ce\",\"certain\",\"certainly\",\"cf\",\"cg\",\"ch\",\"changes\",\"ci\",\"cit\",\n",
    "                \"cj\",\"cl\",\"clearly\",\"cm\",\"c'mon\",\"cn\",\"co\",\"com\",\"con\",\"concerning\",\"consequently\",\"consider\",\"contain\",\n",
    "                \"containing\",\"contains\",\"corresponding\",\"couldn\",\"couldnt\",\"couldn't\",\"cp\",\"cq\",\"cr\",\"cry\",\"cs\",\"ct\",\"cu\",\n",
    "                \"currently\",\"cv\",\"cx\",\"cy\",\"cz\",\"d\",\"d2\",\"da\",\"date\",\"dc\",\"dd\",\"de\",\"definitely\",\"describe\",\"described\",\n",
    "                \"despite\",\"detail\",\"df\",\"didn\",\"didn't\",\"dj\",\"dk\",\"dl\",\"doesn\",\"doesn't\",\"don\",\"don't\",\"down\",\"downwards\",\n",
    "                \"dp\",\"dr\",\"ds\",\"dt\",\"du\",\"due\",\"dx\",\"dy\",\"e\",\"e2\",\"e3\",\"ea\",\"each\",\"ec\",\"ed\",\"edu\",\"ee\",\"ef\",\"effect\",\"eg\",\n",
    "                \"ei\",\"eight\",\"eighty\",\"ej\",\"el\",\"eleven\",\"else\",\"elsewhere\",\"empty\",\"en\",\"end\",\"ending\",\"entirely\",\"eo\",\n",
    "                \"ep\",\"eq\",\"er\",\"es\",\"especially\",\"est\",\"et\",\"et-al\",\"etc\",\"eu\",\"ev\",\"everybody\",\"everyone\",\"everything\",\n",
    "                \"ex\",\"exactly\",\"except\",\"ey\",\"f\",\"f2\",\"fa\",\"fc\",\"few\",\"ff\",\"fi\",\"fifteen\",\"fifth\",\"fify\",\"fill\",\"fire\",\n",
    "                \"fix\",\"fj\",\"fl\",\"fn\",\"fo\",\"followed\",\"following\",\"former\",\"formerly\",\"forth\",\"forty\",\"four\",\"fr\",\"front\",\n",
    "                \"fs\",\"ft\",\"fu\",\"further\",\"furthermore\",\"fy\",\"g\",\"ga\",\"gave\",\"ge\",\"gi\",\"give\",\"gives\",\"giving\",\"gj\",\"gl\",\n",
    "                \"goes\",\"gone\",\"gotten\",\"gr\",\"greetings\",\"gs\",\"gy\",\"h\",\"h2\",\"h3\",\"hadn\",\"hadn't\",\"happens\",\"hardly\",\"hasn\",\n",
    "                \"hasn't\",\"haven\",\"haven't\",\"hed\",\"he'd\",\"he'll\",\"hello\",\"hence\",\"here\",\"hereafter\",\"hereby\",\"herein\",\n",
    "                \"heres\",\"here's\",\"hereupon\",\"herself\",\"he's\",\"hh\",\"hid\",\"himself\",\"hither\",\"hj\",\"ho\",\"hopefully\",\"howbeit\",\n",
    "                \"however\",\"how's\",\"hr\",\"hs\",\"http\",\"hu\",\"hundred\",\"hy\",\"i2\",\"i4\",\"i6\",\"i7\",\"i8\",\"ia\",\"ib\",\"ibid\",\"ic\",\"id\",\n",
    "                \"ie\",\"ig\",\"ignored\",\"ih\",\"ii\",\"ij\",\"il\",\"i'll\",\"i'm\",\"immediate\",\"immediately\",\"importance\",\"important\",\n",
    "                \"inasmuch\",\"inc\",\"indeed\",\"index\",\"indicate\",\"indicated\",\"indicates\",\"information\",\"inner\",\"insofar\",\n",
    "                \"instead\",\"interest\",\"invention\",\"inward\",\"io\",\"ip\",\"iq\",\"ir\",\"isn\",\"isn't\",\"it'd\",\"it'll\",\"it's\",\"iv\",\n",
    "                \"i've\",\"ix\",\"iy\",\"iz\",\"j\",\"jj\",\"jr\",\"js\",\"jt\",\"ju\",\"k\",\"ke\",\"kept\",\"kg\",\"kj\",\"km\",\"known\",\"knows\",\"ko\",\n",
    "                \"l\",\"l2\",\"la\",\"largely\",\"last\",\"lately\",\"latter\",\"latterly\",\"lb\",\"lc\",\"le\",\"les\",\"lest\",\"lets\",\"let's\",\n",
    "                \"lf\",\"liked\",\"likely\",\"line\",\"lj\",\"ll\",\"ll\",\"ln\",\"lo\",\"looking\",\"los\",\"lr\",\"ls\",\"lt\",\"ltd\",\"m\",\"m2\",\"ma\",\n",
    "                \"mainly\",\"may\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"mightn\",\"mightn't\",\"mill\",\"miss\",\n",
    "                \"ml\",\"mn\",\"mo\",\"moreover\",\"most\",\"mostly\",\"mr\",\"mrs\",\"ms\",\"mt\",\"mu\",\"much\",\"mug\",\"must\",\"mustn\",\"mustn't\",\n",
    "                \"n\",\"n2\",\"na\",\"namely\",\"nay\",\"nc\",\"nd\",\"ne\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"needn\",\"needn't\",\n",
    "                \"neither\",\"nevertheless\",\"ng\",\"ni\",\"nine\",\"ninety\",\"nj\",\"nl\",\"nn\",\"non\",\"none\",\"nonetheless\",\"noone\",\"nor\",\n",
    "                \"normally\",\"nos\",\"noted\",\"nothing\",\"novel\",\"nowhere\",\"nr\",\"ns\",\"nt\",\"ny\",\"o\",\"oa\",\"ob\",\"obtain\",\"obtained\",\n",
    "                \"obviously\",\"oc\",\"od\",\"often\",\"og\",\"oi\",\"oj\",\"ok\",\"okay\",\"ol\",\"old\",\"om\",\"omitted\",\"ones\",\"onto\",\"oo\",\"op\",\n",
    "                \"oq\",\"ord\",\"os\",\"ot\",\"others\",\"ou\",\"ought\",\"ours\",\"ourselves\",\"overall\",\"ow\",\"owing\",\"ox\",\"oz\",\"p\",\n",
    "                \"p1\",\"p2\",\"p3\",\"page\",\"pagecount\",\"pages\",\"par\",\"part\",\"particular\",\"particularly\",\n",
    "                \"pc\",\"pd\",\"pe\",\"per\",\"perhaps\",\"pf\",\"ph\",\"pi\",\"pj\",\"pk\",\"pl\",\"placed\",\"please\",\"plus\",\"pm\",\"pn\",\"poorly\",\n",
    "                \"possible\",\"possibly\",\"pp\",\"pq\",\"pr\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\n",
    "                \"promptly\",\"proud\",\"provides\",\"ps\",\"pt\",\"pu\",\"py\",\"q\",\"qj\",\"qu\",\"que\",\"quickly\",\"qv\",\"r\",\"r2\",\"ra\",\"rather\",\n",
    "                \"rc\",\"rd\",\"re\",\"readily\",\"reasonably\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\n",
    "                \"related\",\"relatively\",\"research\",\"research-,articl\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"rf\",\n",
    "                \"rh\",\"ri\",\"rj\",\"rl\",\"rm\",\"rn\",\"ro\",\"rq\",\"rr\",\"rs\",\"rt\",\"ru\",\"rv\",\"ry\",\"s\",\"s2\",\"sa\",\"say\",\"sc\",\"sd\",\"se\",\n",
    "                \"sec\",\"second\",\"secondly\",\"section\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"self\",\"selves\",\"sensible\",\n",
    "                \"seriously\",\"seven\",\"several\",\"sf\",\"shall\",\"shan\",\"shan't\",\"shed\",\"she'd\",\"she'll\",\"shes\",\"she's\",\n",
    "                \"shouldnt\",\"shouldn't\",\"should've\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"si\",\"side\",\"significant\",\n",
    "                \"significantly\",\"similar\",\"similarly\",\"sincere\",\"six\",\"sixty\",\"sj\",\"sl\",\"sm\",\"sn\",\"somebody\",\"somehow\",\n",
    "                \"somethan\",\"sometime\",\"somewhere\",\"sorry\",\"sp\",\"specifically\",\"specified\",\"specify\",\"specifying\",\n",
    "                \"sq\",\"sr\",\"ss\",\"st\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"such\",\"sufficiently\",\"suggest\",\n",
    "                \"sup\",\"sy\",\"system\",\"sz\",\"t\",\"t1\",\"t2\",\"t3\",\"taken\",\"taking\",\"tb\",\"tc\",\"td\",\"te\",\"tell\",\"ten\",\"tends\",\n",
    "                \"tf\",\"th\",\"thank\",\"thanks\",\"thanx\",\"that'll\",\"that's\",\"that've\",\"theirs\",\"themselves\",\"thence\",\n",
    "                \"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"there's\",\"thereto\",\n",
    "                \"thereupon\",\"there've\",\"these\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"thickv\",\"thin\",\"third\",\"thorough\",\n",
    "                \"thoroughly\",\"thou\",\"thoughh\",\"thousand\",\"three\",\"throug\",\"thru\",\"thus\",\"ti\",\"til\",\"tip\",\"tj\",\"tl\",\"tm\",\n",
    "                \"tn\",\"together\",\"took\",\"top\",\"toward\",\"towards\",\"tp\",\"tq\",\"tr\",\"tried\",\"tries\",\"truly\",\"ts\",\"tt\",\n",
    "                \"tv\",\"twelve\",\"twenty\",\"tx\",\"u\",\"u201d\",\"ue\",\"ui\",\"uj\",\"uk\",\"um\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\n",
    "                \"unlikely\",\"until\",\"unto\",\"uo\",\"upon\",\"ups\",\"ur\",\"useful\",\"usefully\",\"usefulness\",\"ut\",\"v\",\"va\",\"value\",\n",
    "                \"various\",\"vd\",\"ve\",\"ve\",\"viz\",\"vj\",\"vo\",\"vol\",\"vols\",\"volumtype\",\"vq\",\"vt\",\"vu\",\"w\",\"wa\",\"wasn\",\"wasn't\",\n",
    "                \"wed\",\"we'd\",\"we'll\",\"b\",\"we're\",\"weren\",\"werent\",\"weren't\",\"we've\",\"what'll\",\"whats\",\"what's\",\"whence\",\n",
    "                \"when's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"where's\",\"whereupon\",\"wherever\",\n",
    "                \"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whom\",\"whomever\",\"whos\",\"who's\",\n",
    "                \"whose\",\"whys\",\"wi\",\"widely\",\"willing\",\"wish\",\"within\",\"wo\",\"won\",\"wont\",\"won't\",\"words\",\"world\",\n",
    "                \"wouldn\",\"wouldn't\",\"www\",\"x\",\"x1\",\"x2\",\"x3\",\"xf\",\"xi\",\"xj\",\"xk\",\"xl\",\"xn\",\"xo\",\"xs\",\"xt\",\"xv\",\"xx\",\n",
    "                \"y\",\"y2\",\"yet\",\"yj\",\"yl\",\"youd\",\"you'd\",\"you'll\",\"you're\",\"yours\",\"yourselves\",\"you've\",\"yr\",\"ys\",\"yt\",\n",
    "                \"z\",\"zi\",\"zz\",\"ad\",\"again\",\"arent\",\"before\",\"begins\",\"best\",\"better\",\"call\",\"came\",\"come\",\"comes\",\n",
    "                \"considering\",\"course\",\"di\",\"different\",\"done\",\"during\",\"either\",\"em\",\"enough\",\"example\",\"far\",\"five\",\n",
    "                \"follows\",\"full\",\"gets\",\"given\",\"go\",\"hasnt\",\"help\",\"hers\",\"hi\",\"home\",\"itd\",\"itself\",\"keep\",\"least\",\n",
    "                \"let\",\"looks\",\"might\",\"million\",\"move\",\"myself\",\"nobody\",\"other\",\"outside\",\"pas\",\"past\",\"po\",\"potentially\",\n",
    "                \"presumably\",\"quite\",\"really\",\"right\",\"run\",\"same\",\"seems\",\"sent\",\"serious\",\"since\",\"slightly\",\"somewhat\",\n",
    "                \"stop\",\"sure\",\"theyd\",\"throughout\",\"try\",\"twice\",\"under\",\"use\",\"used\",\"uses\",\"using\",\"usually\",\"vs\",\"wants\",\n",
    "                \"wasnt\",\"welcome\",\"went\",\"whatever\",\"whenever\",\"without\",\"wouldnt\",\"youre\",\"yourself\",\"zero\"]\n",
    "\n",
    "#Código que fornece a lista de stopwards \"filtrada\" \n",
    "\n",
    "stopwords = []\n",
    "for i in stopwords_aux:\n",
    "    if i in stopwords_aux and i not in stopwords_lixo:\n",
    "        stopwords.append(i)\n",
    "print(stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça aqui uma descrição do seu produto e o que considerou como relevante ou não relevante na classificação dos tweets.\n",
    "\n",
    "ESCREVA AQUI... O produto que escolhi para construção do algorítimo é o produto de limpeza \"roomba\", um aspirador de pó robô automatizado.O objetivo desse algorítimo é: A partir de tweets que mencionam o produto obter informações sobre a opinião dos usuarios desse produto, a fim de descobrir se o produto mostra relevância e popularidade entre os usúarios da plataforma \"twitter\" e orientar uma hipotética equipe de marketing a partir das informações das opiniões dos usuários coletada por esse algorítimo.\n",
    "    Foram considerados relevantes: menções sobre a qualidade do produto, menções sobre problemas técnicos de usúarios que usam a plataforma, comentarios comparáitivos a produtos semelhantes e comentários que demonstram intenção de compra do produto, assim como elogíos ou críticas específicas ao produto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0006817794443497\n",
      "1075\n",
      "1027\n",
      "1934\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "#separando o dataframe em dois, de acordo com a relevancia feita na coragem.\n",
    "treino_relevante = treino.loc[treino.Relevancia == 1]\n",
    "treino_irrelevante = treino.loc[treino.Relevancia == 0]\n",
    "\n",
    "#O codigo não le o dataframe inteiro a menos que ele esteja exposto, se não ele le apenas o \"resumo\" que aparece no 'print'.\n",
    "#Estas funções fazem com que o conteúdo inteiro seja exíbido. \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "#========================Construção das bases de dados como feito na aula 7========================================\n",
    "\n",
    "lista_relevante = cleanup(str(treino_relevante.Treinamento))\n",
    "lista_relevante = separa_emoji(lista_relevante.lower()) #Adiciona os emojis separados pela função\"separa emojis na lista_Rel\n",
    "serie_relevante = pd.Series(lista_relevante) \n",
    "tabela_relevantes = serie_relevante.value_counts()\n",
    "tabela_relevantes = tabela_relevantes.drop(labels=stopwords) #retira as stopwords da tabela\n",
    "\n",
    "lista_irrelevante = cleanup(str(treino_irrelevante.Treinamento))\n",
    "lista_irrelevante = separa_emoji(lista_irrelevante.lower())\n",
    "serie_irrelevante = pd.Series(lista_irrelevante) #Adiciona os emojis separados pela função\"separa emojis na lista_Irrel\n",
    "tabela_irrelevantes = serie_irrelevante.value_counts()\n",
    "tabela_irrelevantes = tabela_irrelevantes.drop(labels=stopwords)# Retira as stopwords da tabela\n",
    "\n",
    "lista_total = cleanup(str(treino.Treinamento))\n",
    "lista_total = separa_emoji(lista_total.lower())# Adiciona os emojis separados pela função\"separa emojis na lista_total\n",
    "serie_total  = pd.Series(lista_total)\n",
    "tabela_total = serie_total.value_counts()\n",
    "tabela_total = tabela_total.drop(labels=stopwords) # Retira as stopwords da tabela\n",
    "\n",
    "#==========================================================================================================\n",
    "#Calculando as probabilidades da palavra ser relevante ou irrelevante dado o conjunto completo.\n",
    "P_rel = len(serie_relevante)/len(serie_total)\n",
    "P_irrel = len(serie_irrelevante)/len(serie_total)\n",
    "\n",
    "\n",
    "print(P_rel+P_irrel)\n",
    "print(len(tabela_irrelevantes))\n",
    "print(len(tabela_relevantes))\n",
    "print(len(tabela_total))\n",
    "print(len(treino))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Essa funçao calcula a probabilidade de uma dada palavra estar no grupo relevante e de estar no grupo irrelevante.\n",
    "def calcula_probabilidades(palavra):\n",
    "\n",
    "#O trecho do código abaixo previne que o cálculo da probabilidade de determinada palavra se encaixar em um determinado grupo\n",
    "#(relevante ou irrelevante) atinga o valor \"0\". Mesmo que essa palavra não não apareça na base de dados do grupo determinado.\n",
    "#Esse procedimento é conhecido como \"suavização de laplace\" atingimos esse resultado ao atribuir um valor de probabilidade \n",
    "#arbítrario > 0 para todas as palavras aplicadas ao algorítimo para compensar essa adicção no valor da probabilidade\n",
    "#dívidimos a soma resultante pela quantidade total de palvras na \"lista_total\" dessa forma o resultado da divisão nunca será\n",
    "# > 1.\n",
    "    \n",
    "    \n",
    "    smoothiner = 1 #valor arbítrario\n",
    "    \n",
    "    if palavra in tabela_relevantes:\n",
    "        freq_R = tabela_relevantes[palavra]\n",
    "    else:\n",
    "        freq_R = 0\n",
    "        \n",
    "    if palavra in tabela_irrelevantes:\n",
    "        freq_I = tabela_irrelevantes[palavra]\n",
    "    else:\n",
    "        freq_I = 0           \n",
    "            \n",
    "   \n",
    " #============================Aplicação da suavizção de laplace=============================================================\n",
    "    Rel_p = (freq_R + smoothiner) / (len(lista_relevante) + len(tabela_total))\n",
    "    Irrel_p = (freq_I + smoothiner) / (len(lista_irrelevante) + len(tabela_total))\n",
    "    return [Rel_p, Irrel_p] #  Rel_p = P(palavra|Relevantes)  ||  Irrel_p = P(palavra|Irrelevantes)\n",
    "\n",
    "\n",
    "#Essa funçao usa a anterior para calcular a probabilidade de uma dada frase, por palavra, estar no grupo relevante e\n",
    "#de estar no grupo irrelevante. Então as compara e retorna a relevância da frase.\n",
    "def compara_probabilidades(frase):\n",
    "    frase = separa_emoji(cleanup(str(frase)).lower())\n",
    "    P_R = 1\n",
    "    P_I = 1    \n",
    "    for palavra in frase:\n",
    "        P_R *= calcula_probabilidades(palavra)[0]\n",
    "        P_I *= calcula_probabilidades(palavra)[1]\n",
    "    if P_R >= P_I:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#aplicando as funções\n",
    "lista_resultados = []\n",
    "for i in teste['Teste']:\n",
    "    lista_resultados.append(compara_probabilidades(i))\n",
    "\n",
    "teste[\"classificação\"] = lista_resultados\n",
    "print(lista_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verdadeiros positivos:  33.15 %\n",
      "Falsos positivos:  18.92 %\n",
      "Verdadeiros negativos:  31.08 %\n",
      "Falsos negativos:  16.85 %\n",
      "----------------------------------------\n",
      "Acertos:  64.23 %\n",
      "Erros:  35.77 %\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\"comparados\" é uma tabela comparativa dos valores de relevancia atribuidos manualmente e pelo classificador\n",
    "comparados = pd.crosstab(teste['Relevancia'],teste['classificação'], normalize='index')\n",
    "\n",
    "#separando os valores da tabela \n",
    "pos_true = comparados[1][1]/2\n",
    "pos_false = comparados[1][0]/2\n",
    "neg_true = comparados[0][0]/2\n",
    "neg_false =comparados[0][1]/2\n",
    "\n",
    "print(\"Verdadeiros positivos: \", round(pos_true*100,2), \"%\")\n",
    "print(\"Falsos positivos: \", round(pos_false*100, 2), \"%\")\n",
    "print(\"Verdadeiros negativos: \", round(neg_true*100,2), \"%\")\n",
    "print(\"Falsos negativos: \", round(neg_false*100, 2), \"%\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(\"Acertos: \", round((pos_true+neg_true)*100,2), \"%\")\n",
    "print(\"Erros: \", round((pos_false+neg_false)*100,2), \"%\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Concluindo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Comparativo qualitativo:\n",
    "\n",
    "A partir da análise quantitativa da tabela de resultados, pode-se concluir que o classificador apresenta uma performance mínimamente satsfatória, demonstrando um gráu de precisão razoável com bastante espaço para aperfeiçoamento. No entanto, para a realização da atividade proposta esse algorítimo se mostra como uma ferramenta bem utíl, visto que a taxa de falsos tweets relevantes se mostrou baixa (18.92%\"). Ou seja, utilizando essa ferramenta é bem impróvavel que o usúario se deparece com muitos tweets irrelevantes durante sua pesquisa (que é a parte mais cansativa desse processo de análise).O maior problema dessa aplicação é sua alta taxa de falsos tweets irrelevantes que se demonstram um problema grave visto o fato que o algorítimo veta informações relevantes ao usuário.\n",
    "    Uma provável causa dos problemas de performance do classificador pode ter sido uma base de dados de treinamento muito pequena para os critérios adotados para o treinamento. Possivelmente com um aumento na quantidade de tweets poderiamos aumentar a taxa de palavaras relevantes  baixo demais na quantidade de twitter de treinamentos manual a fim de melhora as taxas de frequencias das palavras verdadeiramente relevantes,\n",
    "    \n",
    "    \n",
    "### Explicação:\n",
    "   \n",
    "   Não se pode aumentar a base de dados de treinamento do classificador utilizando o próprio classificador, uma vez que nesse caso o critério de seleção da relevância permaneceria inalterado e portanto o que aconteceria é que atribuição de relevância\n",
    "dos novos tweets estaria sujeita a uma mesma performance de acertos do que na condição inicial do classificador sem os novos tweets.\n",
    "    \n",
    "\n",
    "### Outros usos para a aplicação do classificador naive-bayes\n",
    "  \n",
    "   Dentro dos contextos de aplicação para a empresa hipotética, podemos citar o uso do classificador naive-bayes para medir a quantidade de opiniões positivas e negativas em relação ao produto não somente o quanto ele é comentado assim os administradores da em presa poderiam ter uma concepção melhor ainda da adesão do \"roomba\" entre os usúarios da plataforma twitter. \n",
    "   \n",
    "    \n",
    "### Sugestão de melhorias \n",
    "\n",
    "   Para melhorar o código, além de uma maior base de treinamento ou um critério mais claro, pode-se pensar em analisar frases com dupla-negativa ou sarcasmo. Seria necessario analisar a montagem das frases e quantidade de uso de expressões negativas por tweet no primeiro caso, e analisar. A partir dessa iteração poderiamos criar subcategorias na classificação da relevância como comentários positivos e negativos em relação ao produto levando em consideração o sarcasmo. Assim possibilitando extrair mais informações dos tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separações dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfeiçoamento:\n",
    "\n",
    "Trabalhos que conseguirem pelo menos conceito B vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* IMPLEMENTOU outras limpezas e transformações que não afetem a qualidade da informação contida nos tweets. Ex: stemming, lemmatization, stopwords\n",
    "* CORRIGIU separação de espaços entre palavras e emojis ou entre emojis e emojis\n",
    "* CRIOU categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante. Pelo menos quatro categorias, com adição de mais tweets na base, conforme enunciado. (OBRIGATÓRIO PARA TRIOS, sem contar como item avançado)\n",
    "* EXPLICOU porquê não pode usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* PROPÔS diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* SUGERIU e EXPLICOU melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* FEZ o item 6. Qualidade do Classificador a partir de novas separações dos tweets entre Treinamento e Teste descrito no enunciado do projeto (OBRIGATÓRIO para conceitos A ou A+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**\n",
    "\n",
    "Referencia para a funçao de limpeza de sequências de controle (e outros):\\\n",
    "https://pt.stackoverflow.com/questions/289388/como-remover-n-de-um-string-em-python\n",
    "\n",
    "Referencia da biblioteca utilizada para criar a função que fornece a tabela de validação:\\\n",
    "[pandas.crosstab](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html)\n",
    "\n",
    "Referencia para função de separação dos Emojis:\\\n",
    "https://stackoverflow.com/questions/49921720/how-to-split-emoji-from-each-other-python\n",
    "\n",
    "\n",
    "Referencia para criação da lista stopwords_aux:\\\n",
    "[Maurotoro](https://gist.github.com/sebleier/554280)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
